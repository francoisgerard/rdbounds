% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rdbounds.R
\name{rdbounds}
\alias{rdbounds}
\title{Manipulation Robust Regression Discontinuity Bounds Estimation}
\usage{
rdbounds(y, x, covs = NULL, treatment = NULL, c = 0,
  discrete_x = FALSE, discrete_y = FALSE, bwsx, bwy = NULL,
  kernel = "triangular", orders = array(1, dim = c(2)),
  evaluation_ys = NULL, ymin = NULL, ymax = NULL, type = "ate",
  percentiles = NULL, num_tau_pairs = 50, refinement_A = FALSE,
  refinement_B = FALSE, right_effects = FALSE, yextremes = NULL,
  num_lambdas = 50, num_bootstraps = c(100, 20), Kn = NULL,
  alpha = 0.05, potential_taus = NULL, parallelize = TRUE,
  progressFile = NULL, warningsFile = NULL, kernel_y = NULL,
  bwsxcov = NULL, bwycov = NULL, CDFinputs = list(original = NULL,
  CIsetup = NULL))
}
\arguments{
\item{y}{specifies the outcome/dependent variable. Required.}

\item{x}{specifies the running variable that determines eligibilty for treatment. Required.}

\item{covs}{specifies covariates to implement the covariate-based refinement. Expected as a single factor variable.}

\item{treatment}{specifies the treatment status variable if implementing a Fuzzy RDD. Defaults to computation of Sharp RDD results only.}

\item{c}{specifies the threshold for assignment to treatment (assigned iff \code{x >= c}). Defaults to 0.}

\item{discrete_x}{Boolean. If \code{TRUE}, treat each value of x as a mass-point for density estimation. Defaults to FALSE.}

\item{discrete_y}{Boolean. If \code{TRUE}, treat each value of y as a mass-point for density estimation. Defaults to FALSE.}

\item{bwsx}{is a vector of bandwidths in x, respectively for 1) estimation of the discontinuity in the density of x at the cutoff; and 2)local polynomial estimation of conditional means. Expects either a single bandwidth to be used for both or a vector of two. Required.}

\item{bwy}{is a bandwidth for density estimation of y, implemented if \code{discrete_y=FALSE}. Required if \code{discrete_y=FALSE}.}

\item{kernel}{specificies a kernel function to be used throughout estimation for x. Choices are \code{triangular}, \code{rectangular}, \code{gaussian} and \code{epanechnikov}. Defaults to \code{triangular}.}

\item{orders}{specifies the order of polynomial regression, for: 1) estimation of the discontinuity in the density at the cutoff (\eqn{\tau} in paper), and 2) local polynomial regressions. Expects either a single integer to be used for both or a vector of two values. Defaults to 1 (local linear regression) for all. Estimation of \eqn{\tau} can only be implemented up to quadratic order if \code{discrete_X=FALSE}.}

\item{evaluation_ys}{an explicit vector of y-values to evaluate CDF's at (and PDF's if \code{discrete_y = FALSE}). If \code{evaluation_ys} is not set, the set of unique values of y in the sample will be used. Caution is required if \code{discrete_y=TRUE}, because computation will assume a probability mass function can be estimated from differences in estimated CDF's at subsequent values of \code{evaluation_ys}. This can bias FRD estimates if \code{evaluation_ys} does not contain all values in the support of y.}

\item{ymin}{left/lower bound on y at which to implement a boundary kernel correction if \code{discrete_y=FALSE} and \code{y} is a variable with bounded support (e.g. after censoring). Defaults to \code{NULL}, meaning no boundary kernel correction is implemented on the left side of the support of \code{y}.}

\item{ymax}{right/upper bound on y at which to implement a boundary kernel correction if \code{discrete_y=FALSE} and \code{y} is a variable with bounded support (e.g. after censoring). Defaults to \code{NULL}, meaning no boundary kernel correction is implemented on the left side of the support of \code{y}.}

\item{type}{"ate" for average treatment effects (default) or "qte" for quantile treatment effects at the percentiles given by parameter \code{percentiles}. Defaults to \code{ate}.}

\item{percentiles}{vector of percentiles at which to asses quantile treatment effects. Defaults to median (.5). User may add -1 as a percentile, in order to estimate average treatment effects along with QTE's. For example, \code{percentiles=c(-1,.3,.5)} will compute ATEs as well as the 30 percent and 50 percent QTEs}

\item{num_tau_pairs}{integer number of points to search over in the set of possible values for \eqn{(\tau_0, \tau_1)} in notation of paper, for fuzzy RD estimation. Defaults to 50. If set to 1, the single tau is set to the "rightmost" (t=1) extreme of the set \eqn{T}, such that user can enforce the assumption that always-assigned units always receive treatment (see below), if this is consistent with data.}

\item{refinement_A}{Boolean. If \code{TRUE}, additionally calculate refined bounds with the restriction that always assigned units are at least as likely to be treated as potentially assigned units (i.e. \eqn{\tau_1\ge \tau}; see Corollary 1 in paper) Defaults to FALSE.}

\item{refinement_B}{Boolean. If \code{TRUE}, additionally calculate refined bounds for \code{right_effects} with the restriction that always assigned units on the right side of the cutoff are always treated (i.e. \eqn{\tau_0=0}; see Corollary 2 in paper) Defaults to FALSE.}

\item{right_effects}{boolean. If set to \code{TRUE}, additionally estimate causal effects for units just to the right of the cutoff. Defaults to \code{FALSE}.}

\item{yextremes}{extreme values \eqn{Y_L} and \eqn{Y_U} to assume if \code{right_effects=TRUE}, e.g. \code{yextremes=c(0,100)}. Defaults to the sample range of \code{y}.}

\item{num_lambdas}{integer number of points to search over for the causal effect of units just to the right of the cutoff (lambda in paper). Defaults to 50.}

\item{num_bootstraps}{A vector of the number of bootstrap resamples desired, where the first component is the number of bootstrap samples for estimating confidence intervals, and the second is the number of samples for diagnostic testing of the estimated discontinuity in the density at the cutoff. If a scalar is given, the same number is used for both. Defaults to \code{num_bootstraps = c(100,20)}. To avoid bootstrap testing altogether, set \code{num_bootstraps=NULL} or \code{num_bootstraps=c(0,0)}.}

\item{Kn}{a hardcoded constant for \eqn{\kappa_n} (see Section 5.2 on inference in paper). Defaults to \eqn{log(n)^{1/2}}, where n is the number of observations.}

\item{alpha}{sets the level for confidence intervals. Defaults to alpha=.05 for 95 percent confidence intervals.}

\item{potential_taus}{vector of different values of \eqn{\tau} to use for the confidence intervals estimating the potential impact of manipulation, e.g. \code{potential_taus=c(.025, .05, .1, .2)}.}

\item{parallelize}{indicates whether to parallelize bootstrap computations across the available number of cores on machine, minus one. Defaults to \code{TRUE}.}

\item{progressFile}{a file to output progress to (useful if \code{parallelize=TRUE} and the individual cores can't write to screen). File will be appended to.}

\item{warningsFile}{a file to output full warning messages to from bootstrap estimation if \code{parallelize=TRUE}). File will be appended to.}

\item{kernel_y}{allows a separate kernel for density estimation of \code{y}. Same choices as kernel for \code{x}. Defaults to kernel specified for use with \code{x}.}

\item{bwsxcov}{an optional separate \code{bwsx} to use for quantities that are computed on a subsample conditioned on a value of \code{covs} (e.g. covariate-conditional CDFs).}

\item{bwycov}{an optional separate \code{bwy} to use for quantities that are computed on a subsample conditioned on a value of \code{covs} (e.g. covariate-conditional CDFs).}

\item{CDFinputs}{optional, the \code{rdbounds$CDFinputs} object from a previous run of \code{\link{rdbounds}} on the same dataset. This can be used to speed up processing by allowing CDF and PDF estimation to be skipped on a second run.}
}
\description{
This function implements the estimation procedure in Gerard, Rokkanen, and Rothe (2018) to estimate bounds on treatment effects under potential manipulation of the running varible. Returns an \code{rdbounds} object, which can then be passed to \code{\link{rdbounds_summary}} and \code{\link{rdbounds_export}}.
Note on refinements: "Refinement A" calculates bounds based on the assumption that always-assigned units are at least as likely to receive treatment than potentially-assigned units (Theorem 3 in paper). "Refinement B" calculates bounds based on the assumption that always-assigned units \eqn{always} receive treatment (Theorem 4 in paper).
}
\examples{
\donttest{df<-rdbounds_sampledata(50000, covs=TRUE)
rdbounds_est<-rdbounds(y=df$y,x=df$x, covs=as.factor(df$cov), treatment=df$treatment, c=0,
                       discrete_x=FALSE, discrete_y=FALSE,
                       bwsx=c(.2,.5), bwy = .1, kernel="epanechnikov", orders=1,
                       evaluation_ys = seq(from = 0, to=23, by=.2),
                       refinement_A=TRUE, refinement_B=TRUE,
                       right_effects=TRUE, yextremes = c(0,23),
                       num_bootstraps=0)
rdbounds_summary(rdbounds_est, title_prefix="Sample Data Results")}
}
\references{
Francois Gerard, Miikka Rokkanen, and Christoph Rothe (2016)."Bounds on Treatment Effects in Regression Discontinuity Designs under Manipulation of the Running Variable, with an Application to Unemployment Insurance in Brazil". NBER Working Paper 22892.
}
\keyword{RDD,}
\keyword{discontinuity}
\keyword{discontinuity,}
\keyword{manipulation}
\keyword{manipulation,}
\keyword{regression}
\keyword{robust}
